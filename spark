apply deviation from equilibrium idea as opp model to MCTS?  Will this defeat the point?

how would the glass bead game look in real life.  what language communicates all those different ideas?

read about the abc theorem.  structure of primes seems to be a crucial entelechy

entelechy: the prime, vital force behind an orgamism or other organization of information

thesis: do mcts + dbbr with naive EV calculator in lieu of equilibrium strat
	will the mcts overcome the naivety of the EV calc
	can evolve smarter strategies by repeated play, custom tailor profiles for specific types of players

make presentation and send it to the profs

get formalized with respect to paper work


Looks as though the naive opponent model is a half-baked idea.  For ex, how to estimate payouts under this model?  Just going by pocket probabilities will give same payout for every action history that reaches showdown.... So will have to do some learning from hand histories, at least to get a rudimentary model down.

How did the EM mutual learning of P(Holding|History,Action) and P(A|History,Holding) work with EM of k-models learning?

parvenu: one that has recently or suddenly risen to an unaccustomed position of wealth or power and has not yet gained the prestige, dignity, or manner associated with it

the ending to joseph knetch's story was crushing, heroic, tragic, inevitable, comical, defeating, inspiring


Got big speed ups by profiling and reworking MCTS interface to only ask DOMAIN to do the minimum necessary work

Toby will be a LOT of work

Symmetry though, who is to say reflection and rotation (miracle and wonder) are the only 4 types.  What about other functions, whose repeated functional-folding on fundamental area produces the original after k steps.  Can that be thought of as symmetry?


Go scoring: is a win a win (+1) or should keep track of score (current)?  Keeping score gives weight to blowout, which might skew node choice in the early stages

9/26/12

For poker, what should the exploration constant be?  How to pick a good one? What did lit say?
REREAD Monte-Carlo Tree Search in Poker using Expected Reward Distributions
Put on kindle:
Integrating Opponent Models with Monte-Carlo Tree Search in Poker


Start porting mcts_go to C.  But first do some experiments with leaf parallelization in python

10/3/

Simulation showdowns:
Don't want to go the learn P(H|actions, history) via the bayes P(action| H, history, actions) route.  I think it is overly complicated and hard to evaluate.  Rather would like to learn a classifier that takes the game description and outputs a distribution over holding.  Problem is the 1326 output categories (more for omaha).  kNN?

Or could output a bucket, but then the question is how to sample pockets from that bucket given the flop?  Cannot store in memory too big, cannot go to disk, because simulating in MCTS needs to be crazy fast.

The other MCTS opp model paper seems to have a way around this with their differentiating function approach.

We want to assign a probability to P(x|Dp), where x is game state (under the betting abstraction) and Dp is the distribution representing some player (in my case a conglomeration of players, I am trying to learn a basic opp model, not fit specifically).  So x will be the board, action history, and hole cards, and we would like to know how likely it is that p would play in such a way.

Using Bayes, they reduce this to a function depending only on P(Dp|x) and P(x|D*).  The prior we basically make up, could be uniform, ie no matter what the action history every pocket pair is equally likely.  P(Dp|x) is the differentiating function, its job is to take a random example and output the prob it should be labeled either uniform or player.

This done with decision trees.  We dump an equal number of examples from Dp and D*, define a feature space computed on x, and let the tree build.  Now we can plug in a new example, let is plinko down the tree.  The leaf it falls in will contain some number of p training examples and some number of * training examples, this gives us P(Dp|x).  

Now we can query P(x|Dp).  To figure out distribution over hole cards, fix the action hisotry and board to whatever the simulation determined, then vary the hole card description.  Plinko through the model and we can sample.  But how, run all 1326 through, get probs, then sample?  This is slow, would like to do without the upfront comp somehow...

The important part will be defining a good feature set.  Hand strength bucketing helps here?  Otherwise potential hand strength numbers far exceeds the number of training examples


10/8

Trying to compute E(HS^2).  Normally you take a hand, board, and rollout all possible dealings.  For each rollout, you iterate over every other holding and compute the win, tie, and lose number.  Square this number and average over the number of rollouts.

I cannot currently do this with pokerEval.  I can get the hand strength of H1 vs H2 over all rollouts.  And the HS of H1 vs H3 over all rollouts.  I guess I can square this number to get another E(HS^2).  What are the differences or tradeoffs here?  I don't think it is capturing what I want, which is that draw-y hands get a higher score than non-draw-y hands.

So the question is how awkward it will be to get the real deal?  Will have to do:
pockets=[H1, ['__','__']] over all the possible boards.

------

Why write P(An| A1...An-1, C) when P(C | A1...An-1 ).  Guessing the cards is dependent on the actions.

Can't learn P(An|An-1,C) from the data, it is biased towards good cards, will never predict a fold.  We can learn this way to predict showdowns, because it is already a given the players never folded...

What about doing self play?  This way you can know the hole cards, will repeated play/learn/play update cycles converge to a stable,  improved player?  Also can play against Poki in Academy?

10/9

Met with Elkan.  

For bet size learning, just eyeball the distribution, no need to do MOG

For opp model, is there any existing model to be used?  We just need a reasonable prior.
Perhaps can use Poki?  Is it fast enough?  If we do use poki, how do we get a card predictor for showdowns.

Email authors of UCT+ about details.  Why the complicated recursive definition of value?  What about when the sim leaf is not a game leaf (only one data point...)?  

10/12

10/14

Java or C++ for bot.  Decided on C++, but have to integrate with Java to play Poki.  Tried JNI interface, but too much bitchwork and head banging to be worth it.  Will just fork a new process to run decision maker.  Overhead vs comp time should be minimal.

Variance Reduction in MCTS - ideas to consider

10/15

UCT+ authors emailed back, opentestbest is a working implementation of UCT+.  A lot a machinery to test idea out in.

Proposed Work:

UCT+ via online mean and variance averaging.  Will be worse than maxUCT+, but will be faster.  Does the extra computation give an advantage?

UCT+ but where every child is expanded at once.  If worst child expanded first, and the parent value depends solely on this child, meaning the parent may not get selected now.  Also do where the simulation for each new node gets done j times, not just once (actually this probably isn't so important).  AHA: in paper, they say UCT is used for the first 200 samples, the move to +.

Variance reduction techniques, worth it?

Opp model learning: Unclear what authors did insofar as estimating payoffs and predicting actions (just emailed).  Plan to use differentiating function approach, as well as EM approach to predict cards, to then better predict actions

integrate DBBR with the given UCT+ and the given WEKA prior learned by the authors.  This means re-reading DBBR and really knowing it.  Will this converge fast enough to be useful in a game?  Or perhaps will be a way to get a better prior.

DBBR is the most interesting, let's focus on it, enhanced with domain priors on posterior bucket probabilities.  will need learning thru diff function as well.  when it goes to showdown, can use info to feedback loop on these predictions as well.

10/29

Emailing with DBBR author, my understanding of thier notation was off.  My original idea of incorporating showdowns also doesn't work, because changing the P(A|B,N) aka the strategy does alter the P(B|N) after all.  Perhaps on a showdown we could alter the P(B|N) directly as well?

Another idea is using the similarity between distributions to have an 'area of effect'.  Say a player's hand is revealed, giving info on the true buckets. Say this helps us adjust that public info set's P(B|N).  It should also say something about a PIS very close to the original as well.

Similarities will have to be recomputed every kth time the strategy changes.  How can we measure the true usefulness of such a strategy?  Play a fixed opponent and see which one exploits the fastest?

Another question, does this learning method get beat by a dynamic opponent, even one who rotates their strategy deterministically?

Related, does Poki adapt it's play and/or is it static?

==============================================================================================

12/6

TOBY:

Talked with Elkan about learning to predict cards.  He suggested Bayesian network and doing proper EM, not the pseudo-EM with decision tree.

The crux of the matter will be representing a round of betting.  Currently thinking something along the following lines:

totalPIP/BB, totalPIP/effstack, aggPIP/totalPIP, passivePIP/totalPIP.  This way each player gets one node per round that sums up their behavior.  No interaction between bids in the same round, only influence the next.  These seem like 4 good features, but perhaps not the best, or most economical.  Can do decision tree on a superset of features and let it deduce which ones are the best to use.

Given the four mentioned, though, discritizing them will be the next issue.  Can do them each independently.  This leaves a ~7^4 state space though.  Maybe they vary together, can try to cluster and see what comes out.

GO:

Got hired onto Cauwenberghs lab working on Go.  Read up on Temporal Difference learning.  Also, keep working on parallelizing the code.  Can use the 12 cores available, either to do more leaf parallelization and/or tree parallelization.  

==============================================================================================

12/7

Might be some issues with BN approach.  Many examples end preflop, leaving the majority of states hidden.  The EM algorithm must marginalize over these hidden states.  Will have to work hard to 1) limit the number of parents of each node, and 2) make the state space as minimal as possible without sacrificing too much detail.

Next steps:  try to cluster the state spaces of the bidding on each street.  Might have to resort to a very naive fraction of pot PIP measure and nothing else.  Or, rather than aggPIP/totalPIP, just have binary indicator if aggression is more significant than passive.

Still though, state space of flop or river alone is too big to make this tractable.  Will need feature space over board.

==============================================================================================

12/8

New network structure:  for each street, have a board->belief->bid layer.  The bid layers after the preflop interconnect to all subsequent bid layers of all opponents.  We already know the board->belief CPT from precomputed HS2 distributions.

Do not need feature space over board.  Say doing the Mstep parameter update for a river decision node.  When summing over t (training data), only use those examples where the game got to the river street.

Also, starting data sandboxing with bid feature space.  The pip_to_pot seem pretty easily clusterable.  The pip_to_sb is harder, there is a long tail of big outliers.  agg_to_pip and pass_to_pip are overwhelmingly either 1 or 0.  What if weighted by the amount won and lost on each hand.  In other words, hands where both player have a mix of aggression and passivity (i.e both players think they are strong) are probably much more important in the effect on bank roll than someone doing a small raise preflop and winning the hand.

Look into this more getting contradictory readings...

============================================================================================================

1/8/13

Have settled on pip_to_pot ratio, aggressive indicator, last_to_act indicator, as the cursory state space for action nodes.  Have got parsing.py/table.py to spit out the relevant data, separated by street into files.  Next, need to parse out the pockets when they are revealed, then compute the HS2 score and append this data.

With the training examples ready to go, work out all the M step updates. and get to run

Also, will need to be able to go from BeliefBucket + Board -> List of pockets belonging to that bucket.  Will have to run the bucketing procedure again, but explicitly store the pocket:HS2 score per board, not just the HS2 score.  Will take a lot of space?  Better way to do?  Otherwise have to compute HS2 score of every pocket pair and determine in which bucket it falls....

===========================================================================================================================

1/9/13

A lot of the bets preflop are between the .5 and 1 bets we currently allow.  Consider adjusting

=============================================================================================================================

1/14/13

Since we are collapsing all betting rounds into one node in network, consider including "got check-raise", and "did check-raise" binary features in state space.  

Also, what would happen if we introduced a "strategy" state-node that also influenced action?  Say it could take on passive|aggressive.  It would not be labeled in any of the training data.  Would something good come out after training?  Probably not, right?  Needs some kind of toehold.

Decided on a nomenclature for the BN: a12 => the action node at street 1 (flop), for player2
                                      b21 => the hand strength belief of player1 at street 2 (turn)

Once the network is learned, we will want to do inference.  That is, will want to know: What is the belief of player1 about his hand strength at the flop, given player1 has already acted?

P( b11 | a01,a02,a11,flop ) = P( a11 | b11,a02,a01,flop ) x P( b11 | flop,a02,a01 ) / P( a11 | a01,a02,flop )    
                              |______CPT__________||dsep|   |____known___||_dsep__|   SUM_b11{ P( a11 | b11,a01,a02 ) x P( b11 | flop ) } <---marginalized, shown after d-sep

Concerned there won't be enough data given the state space of the actions.

Started working out EM updates in flip notebook.  

TODO: Will need to files that contain all the information up to each of the flop, turn, and river.  The data should go in different files depending on whether or not it had a showdown.
TODO: Some error in running parsing.py
TODO: Get a feel for what percentage of hands end in showdown

==================================================================================================================================

1/16/13

~10% of hands have showdown
Fixed parsing error when all-In actions are taken before the river

Decided spliting the parsed action_states into different files is redundant and unnecessary.  Need inside the correct indexes to be built for efficient computation of CPT updates

#Indexes (built around training_data.txt) :
#   index_last_street.txt
#   want rows(games) who reached the given street
#   {..., ..., 2 : [line num of games that reached the turn], ...}
#   g
#   index_aXY.txt
#   for action_state on street X, player Y
#   {a00 : [[line nums of games matching action_state_value1],[],...],
#    a01 : [[],[line nums of games matching action_state_value2],...],
#     ...}
#   8 keys with list of ~7*2*2 inner lists.  Total ints in inner list ==
#   the number of games in training data
#   
#   Will have to enumerate action_state values in some canonical order to make
#   lists above be meaningful
#   index_action_states.txt
#   "[action,state,value1] : 0,
#   "[action,state,value2} : 1, ... }
#
#   Should rep action_states as the str( list tuple of values )


====================================================================================================================

1/19/13

Try making many features over actions, then doing PCA, and try varying number of components to assess where the increasing in state space size degrades performance so much the gains in accuracy are no longer worth it.

Added arrows between beliefs, e.g b11 and b21.  Ideally, we would like to be able to answer: how likely is bucket X on the turn given for flop F we were in bucket Y?
This would be a beast to store in many files, but fatally slow to lookup in EM updates.  So we can compute general probability, over all flop->turn transitions, of these probs.
On second though, might not be bad.  We want canonical(flop) -> canonical(turn) possibilities.  Could very well fit in mem.

This means will have to re-run bucketing algo, this time let's print a file for each board storing (pocket,EHS2) tuples.  We can bucket with this, and do the above computation.

Considering not implementing EM myself.  matlab package http://bnt.googlecode.com/svn/trunk/docs/usage.html#tabular seems like it will work
	- How to fix certain CPT's to not get updated? (specifically, P(b21|b11) we want to precompute and have static)
		- treat them as root nodes?

=====================================================================================================================

1/23/13

Decided on using Bayes Net Toolbox (MATLAB).  Figured out that CPT's could be clamped, important for the belief nodes, whose CPT's will be computed offline.  Work so far in /home/andrew/downloads/bnt/toby_net.m

Concerned over how I will evaluate such a hole card predictor.  Can leave some revealed hand data out of training, but the whole point is to reasonably predict the unlabeled data.  
	- Trying to find dataset where all the hands are revealed
		-email machine v machine, man v machine tournaments, UofAlberta
		-trying to scrape data from cardrunners


=========================================================================================================================

1/24/13

email bitches about datasets!  very important

============================================================================================================================

1/25/13

Thought of novel approach to labeling data.  Idea: action sequences where someone folds on the flop should correspond to action_sequences that go to showdown, excluding the river itself.  That is, the behavior should be the similar because it is being generated by similar bucket sequences.  At some point, though, the river goes wrong and someone has to fold.

So, we can take the labeled data and k-cluster it by bucket sequences.  Label the action sequences with the cluster labels, and do a decision tree on it (one that is street dependent, i.e test for criteria regarding info up to flop before testing on info up to turn).  Now look at data where the flop has a fold.  Plink that through the decision tree and see which clusters it most resembles.  Draw a bucket sequence (or multiple?) from the combination of these two groups.

Now we have more labeled data for pf, flop, turn sequences.  Do the clustering on sequeces again (what should this look like?), learn a new decision tree of length -1, and send through the examples where someone folded on turn.  Label.  Recurse.

This all hinges on the assumptions that:
1) The bucket sequences cluster relatively distinctly
2) The decision tree can segregate the clusters reasonable well.

TODO: test 1) first.  Might need more than 10,000 hands to do a good job.

-----------

Got distracted trying on figuring out way to compute P(bF,bT | F,T) i.e what are the chances we were in bucket bF at Flop and bT at turn, given specific flop and turn?  Trying to utilize suit symmetry
How about this:  want to compute the transition scores between canonical flop/turn pairs.  So for ABC_h_2f -> ABCX_h_3f, pick a particular flop, make ABCX_h_3f true, and do the specific computation.  Is this the same for all specific choices of flop and turn?

------------

Also, have done bucketing wrong.  Below 678x would map to same canonical, and both results (on and off suit) for 910 would get accumulated.  This washes everything out, we want one 50, one 100, not two 75's for 678_3f.
     pocket    HS2
     910h      100
678h
     910d      50

     910h      50
678d
     910d      100

To solve, maintain an 'already_repped' variable, in this case 678_3s would be placed inside.  In results, we would have '678h'.  Now 678d comes in, sees its canonical form is already represented, and does not add its d_pocket_HS2 data to the results accumulator.  (Remember only want to accumulate data from boards starting with exactly '678h')

Remember to go ahead and save (pocket,EHS2) pairs this time.  Perhaps instead of saving suit, we save 's1' and 's2' when the hole card suit matches the salient suit of the board.  Might this help solve the problem above?  MMM not really still too complicated
================================================================================================================================

1/26/13

Need some functions to sandbox different bucketTransistionProbabilities.  Need a function to get {pocket:EHS2} for a given board.  Altered mapReduceComputeEHS2() to do it, while maintain the properties it needed to do eventual full-bore bucketing.

Now need function to bucket these {pocket:EHS2} according to some percentiles.

Then need function to take two such {pocket:bucket probs} dictionaries, one for each of flop and turn, and compute the prob that a hand was in bF and bT.

P( bF,bT | F, T) = P( bT | bF,T,F ) * P( bF | F ) 
                         [A]             [by construction of percentiles]

bF <- F   
/\    |
p     |
\/    \/  
bT <- T

[A] P( bT | bF,T,F ) = sum_pockets P(bT,p|bF,T,F) = sum_p P(bT|p,T)                     *  P(p|bF,F)
                                                          [membership of p in bT)]           [B]

[B] P(p|bF,F) = P(bF|p,F)                    * P(p|F)                    /     P(bF|F)
              [lookup in membership probs]    [0or1 if legal assignment]      [by construction of percentiles]

Again, big pictures:  we would like to show that the bucket transistion probs (BTP's) between all legal pairs in canonical(F) and canonical(T) are equivalent (or at least very close).  This is not enough though, becaus there are 15k turns and 38k rivers, ~500,000,000 pairs, too big for CPT.
So hoping that the cardinality independent part is enough, i.e 234_s_2f -> 2348_s_3f is roughly the same as 567_s_2f -> 567Q_s_3f

===========================================================================================================================================

1/28/13

Weeeee data from past acpc comp available.  got a butt load of it

need to get the rules clear, format seems pretty self explanatory

no limit is Doyle's game, everyone cashes out after every hand, then cashes back in with 1000 at the next start

Game	                No-Limit Texas Hold'em (Doyle's Game)
Competition Format	Series of heads-up duplicate matches
Hands Per Match	        3000
Stack Sizes	        200 big blinds (400 small blinds)
Bet Sizes	        No limit
Blind Sizes	        50/100
Blind Structure	        Reverse blinds, no ascending blinds
Showdown Mucking	No
Illegal Actions	        Any illegal action is interpreted as a call if raise is illegal. If it is a raise for an illegal amount, it is interpreted as the closest possible raise amount.

Winner Determination

Bankroll instant run-off and total bankroll

===========================================================================================================================================

2/2/13

Done with parsing ACPC files, need to double check everything is correct, though

Don't think it makes sense to learn one big network.  Instead, let's learn the 4-level network, only looking at hands that make it four rounds.  
Use this model to label the unlabeled examples (where someone folded on river).  Using this true+estimated label set, turn attention to 3-level hands and label hands where someone folded on turn. Rinse repeat.

This should help eliminate bias?  Check accuracy, street and global, after each level.

Quesiton:  how do we instantiate CPT for each round?  First should be random?  Then do we use the CPTS from the previous round as the starting point?  Seems reasonable.

======================================================================================================

2/3/13

Can compute bucket transition probabilities.  Unfortunately, the distribution changes, sometimes (what seems like) signifcantly, among the groups.
For instance 238Q_h_2fxoox -> 238TQ_h_3fxooxx and 239Q_h_2fxoox are noticable different.  However, the change between 'xoox' and 'xxoo' seems ignorable.

We could group manually, but why not use a clustering algorithm.  Each row is a tuple, each number is a bucket -> bucket' prob.  Given k, we assign each row a group.  We compute the average trans probs for the group, and use that for the entire group.

First step, though, is generate the d_pocket_EHS2 for each board.  This is now running on server

Next, want to check Bayes Net matlab for inference capability.  FOr instance, if we ask it for P(a42,a31,a32,b42 | a31,a32,board ), can it do anything?  Quickly?  Will be important as part of the EM algo.

=====================================================================================================

2/4/13

Checked on bucketing, gotten very slow.  I was setting up and tearing down a multiprocessing.Pool each mapReduce, this was stupid.  Instead, passing in the same Pool object to every mapReduce call has sped things up considerable.  Remains to be seen if it doesn't also take longer and longer.  Hovering right around 6s / 100 boards, fuck slowly creeping upwards though  whyyyyyyyy

testing out inference in bayes net.  Using Dirichlet random for CPTs.  fixed matrix order bug in node_sizes.  

===================================================================

2/5/13 

Fuck the fancy bucketing, doing it the straightfoward way, one call to mapReduce for each flop, turn and river.  Proceeding well without any slow down.

======================================================================

2/6/13

BNT Engine seems handle inference queries easily, next step is to load fully labeled data in, learn structure, and test prediction.
Then try labeled+unlabeled and test on more unlabeled.  Except bad performance as they are from different distributions.

Idea: Go over labeled data, and find examples were one of the player very clearly should have folded (the bucket discrepancy was huge).  Generate a new identical tuple, but one where the player has folded instead.  Learn on these examples, + the true unlabeled data and see if better.

Also, for bucket transition probs, generating a file with 35k * 18k lines with turn_buckets * river_buckets floats on each line is too much.  Rather, randomly sample K pairs and use these to generate clusters.  Then do a NN algorithm to label the remaining pairs.

----

Ha EHS2 dist comps already done!  We want to: 
	1) Pick a random board cboard from file.
	1') figure out what actual board it was computed from (can do ahead of time)
	2) Load its d_pocket_bucket from file
	3) enumerate all possible next cards -> collapse them
	4) pick a representative from each collapsed set and compute it's d_board_bucket.
	5) Take the two d_board_bucket's and compute the transition probabilities.
	6) Print: cboard,cboard',transition distribution, to file
	7) if i < K, go to step 1)

Before that, slight error in river collapsing, why getting TTTJK_t_3fxxxxx.hsdist?  Generated by ["Th", "Jh", "Kh", "Td", "Tc"]	

===============================================================================================================================

2/7/13

Fixed 'xxxxx' error.  Does not seem systemic

Correction to 1/26 computation:

P( bT | bF,T,F ) = sum_p P(bT|p,T) * P(bF|p,F) * P(p|F) / P(bF|F)
                            [A]         [B]        [C]      [D]

Was not giving correct answers in pratice.  Turns out C/D is 1/(# of available pockets given flop) * 1/(fraction of pockets in bF).
Ignoring partial bucketing, this is equivalent to 1/(# of pockets in bucket).  So we can just do sum_p (A*B) / sum_p B.  First way is imprecise, because when we are computing the true bucket sizes we are converting floats to ints, and when we tried to recover this information to compute C/D there was some rounding error.  

Very annoying problem with json.  It encodes the d_pocket_buckets as 'pocket' : {'bucket' : prob}.  This makes sampleTransitionProbs awkward, as it is trying to deal with version loaded from file (strings) and those computed by computeBucket (int).  This confuses getTransitionProb.

Find another encoder and use this

=========================================================================================================================

2/8/13

changed from JSON to YAML.  Re-writing the bucketing files on genomequery currently.
What, though, is our final bucket break down going to be?  Right now it is a hand coded distribution.  Can we justify it, encode it more mathematically?  Just uniform doesn't seem right, as we want better discrimination between the high bucket hands, ie the ones that will be bet on, and therefore money can be won or lost on it.  Can just hand wave some exponential argument?
 
to compute the transition probs for a flop to all it's turns takes ~150s.  times 1755 is a few days.  will

Consider moving data to database.  That way it is indexed and less hectic than a bunch of tiny flat files.  Is the time spent doing this worth it?  Prob yes if I am going to have to rerun compute bucketAll again


========================================================================================================================

2/13/13

moved onto DB storage scheme.  

tables to store EHS2 and BUCKETS for each street

loaded compute EHS2 data, computing Buckets directly from DB.  Fastest way:  bucketALLEHS2Dists_DB(): stream ehs2 data from database in an ordered fashion.  chunk it into groups computeBucket can handle.  Write insert statemenst to one big flat file.  Destroy indexes in BUCKET table, do 'load data infile 'file' into table toby.BUCKET;  (First copy file into /var/lib/mysql/toby/)

"mysql> load data infile '<file>' into table toby.<BUCKET> fields terminated by ',';"

very fast:  bucket the flop (2 mil <cboard,pocket> pairs) in 30s.

------------

Talked to charles today

Need to focus on getting SOME kind of model up and working.  No point putting all this time in to transition probabilities if there is no need.  Build basic table and train.

Also, don't think charles understands the game that well, a lot of his advice was very cursory.  Need to have something concrete to show, not just keep talking about board texture.


=======================================================================================

2/15/13

The bucket belief nodes are continuous...  It's a prob distribution over the membership.  Can use Softmax or noisy or for the CPD to represent Belief->Action edge.  This will mean mixing the continuous belief state with discrete action states, how to analyze? Alternatively, to get started, can just take max or weighted avg of the prob_dist and use the winning bucket as the value.  Can also discretize the prob dist into units of 1/n (will make state space too big??)

------------------

Most immediate things to do: 

1) compute EHS2 and buckets for the preflop stage
2) come up with reasonable exponential bucketing percentiles, rebuild the BUCKETING tables on genomequery
3) modify iterateActionStatesACPC to only emit showdown hands
4) Tweak network to remove texture edges
5) Train networked (this will be fully labeled, then test on remaining 10%)
	-how to do network training on server.  prob wont want to be doing on laptop

---------------------

head up preflop EV for 2 player
http://wizardofodds.com/games/texas-hold-em/2-player-game/

----------------------------

wrote exponential.py, generating bulk load files on genomequery to load BUCKET tables.  Collapsed multiple bucket fileds into one CSV field

==================================================================

2/21/13

on step 3

Problem now with bucketing.  

We computed 36K_h_2fxox EHS2 numbers from 3h,3d,Kh
But now we want to know the EHS2 of 3h3c.  Ob this was never computed.
Need a function to generate the symmetric pocket pair, and use that to lookup against the already computed value

new deck::symmetricComplement

=================================================================
2/22/13

Lot of head smashing to get symmetricComplement working well.  Think carefully first like with Goby, then implement.  Right now it is hack after hack to get it to work

I think it is, and it revealed a typo error in collapseBoard.  on the river, different types of 4f were getting collpased to just 4f, rather than 4fxoxxx etc.  Type fixed, rerunning EHS2 dist comp now.

After that, rebuild REPRESENTATIVES and EXPO_RIVER.

Continue from there trying to parse and bucket using symmetry

==================================================================

2/24/13

New EHS2 table loaded.  Yet to bucket.  Cleaned some code up and commented

organized and commented the hackathon that symmetricComplement was/is

=====================================================================

2/25/13

Makes me nervous that EHS2 of 27o is higher than that of 23o.  How does this make sense?  23 should make more straights, which would favor the squaring function, as well as fact that it has a higher hand strength to begin with....

slight bug in computeHS2:

hs = (wins + float(ties/2)) / (wins + ties + losses) old
vs
hs = (wins + float(ties)/2) / (wins + ties + losses) fixed

big deal?  

Bucketing:  In light of 22223_q_r, the bucketing strikes me as off.  Something like Th5d is lumped in with 

Also, wtf, why does pokerEval give slightly different EV results forprint ['Ah','5c'] ['2h','8h','Kh','2d','8d']
['6h','Ad'] ['2h','8h','Kh','2d','8d']


Have to rerun bucketing, sigh, cause the following was happening:
22223_q_r	4hTc	0.324699	0.259259259259:0.740740740741:0:0:0:0:0:0:0:022224_q_r
Turns out was not inserting the trailing \n after the '\n.join(buffer) write


Arg why are the bucket probabilities not coming out to 1????????????

Started constructing simplified test case to investigate
Maybe hold off on this until we get some preliminary results done
-----------------

=======================================================================

3/8/13

Another bug in collapseBoard.  These are painful, because it means the wrong EV dists for a given cboard was computed.

Eg. ['2d', '5s', 'Qs', 'Qd'] was collapsing to 25QQ_p_3f, who is represented by 2h 5h Qh Qd.  Insteas should have mapped to p_22f

Also got 

Go through collapse board with a fine toothed comb and pen and paper.  FIx all remaining errors.

If not too many, is there a way to repair the mistakes without doing the whole extended EV/bucket recomputation?

In the case above, we have introduced some new cboards whose representatives would need to be freshly computed and integrated.  Same for the cboards with incorrect representatives.  

Add a valid, field to REPRESENTATIVES, set all matching p_3f to false.
Also add any new additions to the table with valid = false.

------------------

Fixed the 22f TURN bug and the data is loaded.  Rewrote symmetricComplement after it broke trying to parse.  Conceptually simpler now.  

Hooray just parsed an entire game log file!

------------------------------

TODO: backup DB now that it seems good, dont' want to lose

===================================================================

3/9/13

Do we treat all-ins as checking the useless rounds?  Will be be learning the right thing?  Or should there be another state. We probably should treat it as a k < 4 betting round game where cards are revealed.  This way we do not have to confuse the notion of checking with auto-passing, and do not have to introduce another state for the network to learn (uselessly).  However, right now this is not how things are parsed

Did this...:

So, can now emit only states that go to showdown.  Time for some fully labeled parameter learning

=====================================================================

3/25

Smashing together all runs from a permutation (leaving a few out) as input.
Need to run BNT on the fully labeled data (no board texture edges) and see how well it can predict 4th round actions, or, given actions, predict 4th round buckets.  Actually, predict the actions/buckets of any given round.

Parsing takes a long time.  Database accesses + symmComplement have to be the culprits?  Yep...., a huge difference.  Mostly db, a little symmCompl

---------------------------

another symmComplement 'not enough rows' from DB query error, STILL!
created a log file to catch all of these and not stop

Also, need to be sure of mapping from BNT node values to the correct interpretation.  Will reorder bucket, state number assignments to start from 1, not 0.  Have to do this, otherwise inference engine gives invalid index when trying to lookup row 0 in CPT.  did it

whats the deal, it learned on data with values outside the number of possible values I gave it.  Eg 20 flop bucket values, but toby_net.m specified only 6.  Turns out anything over the top got smashed into the 
top's probability

--------------------------

preliminary training/inference with BNT look promising.  To really tell, need to isolate and fix parsing error described about, then rerun with data 1-indexed instead of 0-indexed

------------------------------------

evaluating accuracy: get distribution for query node.  sum up the probabilities of predicting the true labels.  Harder: capture the notion of closeness for the action_states.  this needs thinking/work, and a finalized conception of action_state (still a ways away)


=========================================================================

3/26/13

More errors with symmComplement....and more errors....

Fleshing out testing code in toby_net

---------------------

Inspecting MLE parameters for fully labelled case just to verify things are interpretable and making sense.  THey are.

--------------------------------

Strategerizing:

Was thinking about reworking the action_states.  Could keep a aggressive/passive ratio instead of the pot fraction.  Does this cluster as nicely as pot fraction.  Would also need to include the multiple of the big blind to have some absolute measure.  Then perhaps an indicator of check_raise, was_check_raised  

How much of a weakness is including no information about >=2 part actions?  i.e the differences between a small and a ludicrously big check raise?

Also, how much is lost not letting the first player's action_state have an edge to the nexts?  Tried to incorporate this with 'in_position' part of the state, but seems lacking.  Trouble is that using one big node to sum the entire round of actions is not fair, because the second player didn't make their sum total decisions based on this info, only a part of it.

But then again, really that bad?  All we are doing computing is: when we see action state X, action state Y is correlated with it by this much.  During the course of a game, player 1 makes the first move, and this automatically precludes some final action_states, and makes other final action states more likely.

If we try including the edge, need to change how the data is parsed.  Right now all of player 1's actions (Rembrant) go to the left node, and all player 2's actions (SartreNL) go the right.  We would want to parse it so the inputs flip flopped (or just include the games were Rembrant was in_position so we are not mixing player action distributions)

----------------------------

Idle question: can we detect different styles of play in a bot when it plays different opponents?

--------------------------------

Want a way to only parse out those games of length N WIHTOUT a showdown, also.  Then can append with/withouts together and not spend so much time parsing

On that note, can we speed up parsing?  that is, speed up the database accesses?

--------------------------------

very preliminary results of predicting the correct bucket are not great: .2828.  More data, more edges, better abstractions

==========================================================

3/27/13

Can parse 100 runs, only 2 symmComplement errors. They both are like this case:

aboard: ['4h', '5h', '7d', '7c', '7s']
pocket: ['4s', '7s']

where mapping s to anything is illegal.

Let's just move on though, fix when there is time

We want parsing to output 4 x 2 x 2 files:
Number of betting rounds x showdown reached x training/test

A little wasteful on space, because we are going to duplicate 4 round games 4 times, in the 1,2,3 round files.  But practically simpler

How do we encode the missing bucket data for MATLAB when the hand doesn't go to showdown?  Do a -1, then translate into empty [] when make into a cell array? Like so:

Pretend we are working on flop data, ie only two rounds of betting

data =

     1     2     3     4
     6     7     8     9
     ...        [_______]
                    |
                 Actions from the flop round for the 2nd game

[ncases,natt] = size(data);
nrounds = natt/2;
cases = cell( 2*natt, ncases );
             for each pair of actions there are two buckets

cases([3 4 7 8], :) = num2cell(data');*
cases = 

     []     []
     []     []
    [1]    [6]
    [2]    [7]
     []     []
     []     []
    [3]    [8]
    [4]    [9]

To get [3 4 7 8]
to_fill = 1:nrounds*4;
to_fill = reshape(to_fill,4,nrounds)';
to_fill = to_fill(:,3:4);
to_fill = reshape(to_fill',1,nrounds*2);

And log2Nodes now only appends bucket data to nodes if must_have_showdown is true

Started to modify logs2Nodes to multiplex output to appropriate file handles, but too sleepy.  Do in morning.

============================================================

3/28/31

Parsing a 100 run into the 16 files.  taking a long time

each run is 3000 hands * 100 = 300,000

2494 @ 23:36
838 @ 23:28    1300 hands in 2 min = 10 hands a second

Can run 4 at a time w no apparent slowdown, though

===================================================================

4/1

Started writing aaai submission

===================================================================

4/2

When evaluating action prediction accuracy, we have equilvalence classes.  All we want to know is check,fold,or pip fraction, not whether we 'predict' in_position or is_aggressive.

is pot odds and implied odds summed up by belief buckets?

also, parsing seems not to have worked.  hugh_lucky7 2-round showdown node file look too long.
Should have:
b,b,a,a,b,b,a,a

but we have many lists of length 16, ah, for all output files, it is smashing two lines together sometimes, probably between file boundaries.  should be fixed

dealing with the issue of including edge between same street action nodes, or making more complicated graph structure:

	Link the two nodes with an edge.
	Remove 'in_position' state
	Fix parsing.  We cant just send playerA to the left side now
	If we have intra round action sequences that is longer than 2, eg. kr1r2c, create two training rows
		..........b1, b2, summarize(k),        summarize(r)       ...............
		..........b1, b2, summarize(k and r1), summarize(r2 and c)...............

NO!!!!!!

will do 4 rounds for the pending actions, summarized actions for the past ones.  See drawing and reasoning in AAAI write-up.  For training examples where there are only two action, just append a 'kk' to the end

Changes to make:

1) parse with directionality in mind.  Let's focus on our accuracy in predicting one player, say SartreNL.  Want to have a file with Sartre in position 1, opponent in postion 2 and also file with positions reversed.
Let's also just focus on 4th round stuff, revealed and not revealed.

2) State space has to change.  Let the summarized nodes be {discretized fraction the pot committed during the round} x {effective_stack_something} x {was_aggressor} x {other order dependent stuff that gets squashed??}, pretty much like they are, with in_position removed.  This is implicit due to the network structure now.  Maybe ignore effective stack for now.  The 4 active action nodes should simply state the pot fraction {also effective_stack, other stuff??}.  toby_net needs to change to reflect this

	Side note: Shouldn't the fractions be computed with regard to the pot after the previous bet was added in?  How does this clustering look?  

Have derivation showing meta-texture is a good thing.  Also, think I found the error in the previous attempt to compute transition probs.  See journal for both.

Describe computation necessary for meta-texture?  We still doing the clustering?  yes i think

==========================================================================================

4/3/13

Changed parsing to take in 'focus_player' and 'player_position', to only parse out those instances where SartreNL is not in position, say.

Reworked toby_net.m to include the two more active_action nodes.  still need to finish quantifying passive vs. active spaces.

Moved table.action_states to table.passive_action_states.  Still need to work in active_action_states.  Included self.actions, which can be used in advanceStreet to map each action to a bet amount.

In node files, it will be better to include -1 where action node is hidden.  I think I can do some MATLAB trickery to turn them into [], see 3/27 for a good starting point

============================================================================================

4/10/13

Fixed parsing bug, a 'cc' would be turned into a 'kc', not 'kk'.  Was misplacement where prev_act was set to 'blinds'
os --------------------------
#TODO
        #now that the network structure has changed, we need to modify this
        #to output expanded active nodes for each possible round
        #the yield statement will go inside the for loop, as will the training
        #instance list
        #e.g a game going 4 rounds will output [past *3*2, 4 active on river],
        # [past 2*2, 4 active on turn], [past *1*2, 4 active on flop], etc..
--------------------------

fixed it so the first player to act always appears on the left side of the network

almost done intifying active action states, just need to extend mapper

made it so unknown values are recorded as a -1, not left out 

then, to make the correct cell arrays in matlab:

data = [-1,-1,1,2,-1,-1,3,4,-1,-1,5,6,-1,-1,7,8,9,10; 
        -1,-1,10,9,-1,-1,8,7,-1,-1,6,5,-1,-1,4,3,2,1]
[ncases,natt] = size(data);
cases = cell(natt,ncases);
visible_ixs = find( data(0,:) >= 0 );
cases( visible_ixs,: ) = num2cell( data(:,visible_ixs)' );
Voila!
cases = 

      []      []
      []      []
    [ 1]    [10]
    [ 2]    [ 9]
      []      []
      []      []
    [ 3]    [ 8]
    [ 4]    [ 7]
      []      []
      []      []
    [ 5]    [ 6]
    [ 6]    [ 5]
      []      []
      []      []
    [ 7]    [ 4]
    [ 8]    [ 3]
    [ 9]    [ 2]
    [10]    [ 1]

===================================================================

4/11/13

Have mixed 2 idea in parsing.py, the parsing of a log file and the emission of nodes.  Parsing should turn string data into the Table representation, and node emission takes the Table as input.  

Final action state representations:
Passive: {pot frac} x {0/1 did_re_raise} x {0/1 was aggressive}
Active: {action} x {pot_frac} (if action c or b, no pot frac component)_

The CPT for the final round nodes are too big to fit in memory!  Consider, 4 past node parents alone is:

(8*2*2)^4 = 2^20 = 1,048,576

But, we realize including the pot fraction for both players is redundant, because it is the same for both.  So if we merge the nodes into one, we could have

(8*2*2*2*2)^2 = 2^14 = 16,384

Still too big for the final action node, a422.  Occurs that we do not need 'b' and 'r' for the action nodes, we just need the amount committed to the pot.  To PIP in row and the network will implicity infer that one is a raise.  THIS IS STILL NOT ENOUGH, out of memory it says

128 * 128 * 11 * 11 * 11 * 10 = 218,071,040 / 11 = 19,824,640 / 11 = 1,802,240 / 11 = 163,840

and actually all these number need another * 11, because thats how many the states themselves have

options:

-take out re-raise indicator from past.  easy.  divide by 16.
-reduce number of betting fractions.  putting to 4 v 8 is 14,049,280 v 218,071,040
-take out betting fractions from past nodes all together
-take out betting fractions from active nodes
-take out edges: which ones?

=========================================================================================

4/12

Even with num_bet_ratios set to 4, still exhausting laptop memory.  Set things up on cadenze with no problem.  Did not try the old value of 8

The problem where enter_evidence gives a 'index OOB' exception is easy: The active_action nodes got int values of 64-90, or whatever, but matlab expects 1-X when I tell it there are X nodes.  Need to set up two actionState2int converters.

Also, set up bash script to copy toby_net.m, training.csv and test.csv to cadenza:/~/bnt

=============================================================================================

During a hand of Texas Hold'em, two cards are dealt to each player.  These hole cards are kept private.  Before any public cards are revealed, a round of betting takes place.  When betting, a player can: check (pass), bet some amount of his remaining chips, call someone else's bet, raise someone else's bet or raise, or fold, forfeiting the chips previously placed in the pot.  Once betting concludes (all players have an equal stake in the pot) and assuming >1 player remains un-folded, the 3 cards are pl

==============================================================================================

4/15/13
On evidence :

[7]    [4]    [10]    [6]    [1]    [4]    [4]    [1]    [4]    []    [1]    [1]    [1]    [1]    [1]
or
[5]    [7]    [9]    [16]    [5]    [6]    [11]    [3]    [6]    []    [3]   [1]    [3]    [4]    [1]

the distributino over buckets is:

ans =

     0
     0
     0
     0
     0
     0
     0
     0
     0
     0

why? Is it because that particular evidence was not in the training?  That should revert to prior though.  Test this hypothesis on toy example...  yes this is indeed the problem.  set to uniform random when this happens.  Turns out that when a few more runs are collected, the number of unseen test examples has dropped to 0.  Large state space may not be a problem

We are still left with the problem of the state space being much larger than the number of data examples.  The last active_node has the largest state space but the smallest data set.  Another approach is needed for these nodes
	We could just take out the connections from the past action nodes, and have their effect be exclusively indirect via the belief edges.  This actually sounds good
Largest size would then be: 10*7^4 = 24,010!  Much better.   (worst will be 20*7^4 for flop, but more examples here)

Trying learn_params_em to see if this suffers the same problem.  Takes much longer, obvs.  It suffers same problem

How can we capture the notion of closeness for belief buckets?  If we get it wrong, are we close?

==========================================================================================

4/16/13

On Rembrant v Sartre, perm1, Sartre in first position ( 3,658,201 train, 40,549 test)
Testing took >10min

avg_chance_of_correct_prediction =

    0.2920


novel_count =

   442

442/40519 = 1.1%, but how much money was exchanged during these hands?

--------
Very interesting.  If I take away the effect of past nodes, it actually improves slightly...

avg_chance_of_correct_prediction =

    0.2942


novel_count =

   359

--------
For predicting the 1st button action on the fourth round, accuarcy is:
avg_chance_of_correct_prediction =

    0.9946


novel_count =

   370

Is this just a matter of there being so many KK actions on the fourth round?  Probably.  Write a sed/awk script to parse out the ith column and do a value break down.  Probably more meaningful is to ask how often it gets it right when the true action is not K.

------------------------------------------------

Boom! Got computeBucketTransitions working.  P(k',k|b,b').  All marginals sum to 1, as does the total.

Running processes to fill up turn->river transitions

Encountered problem with symmComp.  Sometimes a suit has no legal map.  This should only happen on rainbows, in which case we can change the suit of the pocket (still being consistent with board), and see if that gives some legal mapping.  If not, try another suit, rinse repeat

========================================================================================

4/18/13

Why does _test files have ~90 empty lines at the beginning.  Must be from parsing.py??  Have removed manually for the time being

========================================================================

4/19/13

Failed at speeding up trans computations by chunking.  Would have to do things in DB order, not enumeration order.  Leaving for the moment because other things to do

Have toby_text able to eval avg_bucket_distance, and can exclude games that meet some criteria from contribution.  So can exclude all 1,1,1,1 action sequences.  Right now we do straight bucket difference, should we weight it?  It is less important to miss 10 v 9, but a bigger deal to guess 2 instead of 3.  Also, want avg and st.  Perhaps we need to weight each differnce by the amount of money?

In general, it would be good to have amt of money changing hands each round included as the last 'node' (can chop of in matlab) .  That way we can weight, and hopefully learn things, more appropriately.  How does this weighting work, just duplicate expensive hands in the training data?


NOT DOING BETTER THAN baseline, figure out why!!

=====================================================================================

4/19/13

Ok.  I thikn a big reason weren't doing better than baseline was because of the 'k,k' that were getting added as dummies.  They were polluting the ML estimates. See notebook.

I though marking the dummy node with a value +1 of the size of the node would work, but it really just excludes the entire example.  Cant mark it with a [], things break and/or it gets added all to the node's max state.  

So it seems like I need to mark all DUMMY's as [], and do EM.  On toy this was getting the right estimates.  The downside is that this takes much longer...

So, need to take parsing and scale it by dollar amt.  Pick an amt A with that is prob 1, any number higher gets duplicated that many time.  All lower get omitted with prob amt/A.

But try training without scaling first
